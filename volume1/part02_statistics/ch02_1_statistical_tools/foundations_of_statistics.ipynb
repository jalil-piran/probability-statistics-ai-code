{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter: Foundations of Statistics\n",
    "\n",
    "This notebook accompanies the textbook chapter on **Foundations of Statistics**. It contains all Python code examples from the chapter, organized section by section, with markdown explanations for each block of code and its output.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Stem-and-Leaf Display](#stem-leaf)\n",
    "2. [Discrete Histogram – Baseball Hits](#baseball-histogram)\n",
    "3. [Continuous Histogram – Response Times (Bin Rules)](#continuous-histogram)\n",
    "4. [Dot Plot – Response Times](#dotplot)\n",
    "5. [Box Plot – High-Variance Response Times](#boxplot)\n",
    "6. [Sample Mean and Trimmed Mean](#mean-trimmed)\n",
    "7. [Percentiles, Quartiles, and IQR](#percentiles)\n",
    "8. [Variance and Standard Deviation](#variance-std)\n",
    "9. [Location and Variability Comparison](#location-variability)\n",
    "10. [Skewness – Exam Scores](#skewness)\n",
    "11. [Kurtosis – Transaction Amounts](#kurtosis)\n",
    "12. [Outlier Detection – IQR and Z-Score](#outlier)\n",
    "13. [Sampling Distributions Simulation](#sampling-dist)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stem-leaf'></a>\n",
    "## 1. Stem-and-Leaf Display\n",
    "\n",
    "### Section 2.4 – Graphical Summaries\n",
    "\n",
    "A **stem-and-leaf display** splits each data value into two parts:\n",
    "- **Stem**: the leading digit(s), representing the tens place\n",
    "- **Leaf**: the trailing digit, representing the ones place\n",
    "\n",
    "This preserves the exact original values while organizing them in a readable structure.\n",
    "\n",
    "**Dataset:** `[23, 25, 27, 29, 31, 32, 34, 36, 38, 41, 42, 45, 47, 49]`\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sort the data.\n",
    "2. For each value, compute `stem = value // 10` and `leaf = value % 10`.\n",
    "3. Group leaves by stem using a dictionary.\n",
    "4. Print each stem with its sorted leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "data = [23, 25, 27, 29, 31, 32, 34, 36, 38, 41, 42, 45, 47, 49]\n",
    "\n",
    "# Sort the data\n",
    "data.sort()\n",
    "\n",
    "# Build stem-and-leaf structure\n",
    "stem_leaf = {}\n",
    "for value in data:\n",
    "    stem = value // 10\n",
    "    leaf = value % 10\n",
    "    stem_leaf.setdefault(stem, []).append(leaf)\n",
    "\n",
    "# Display stem-and-leaf table\n",
    "print(\"Stem | Leaves\")\n",
    "print(\"-\" * 20)\n",
    "for stem in stem_leaf:\n",
    "    leaves = \" \".join(str(l) for l in stem_leaf[stem])\n",
    "    print(f\"{stem}    | {leaves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "The output:\n",
    "```\n",
    "2 | 3 5 7 9\n",
    "3 | 1 2 4 6 8\n",
    "4 | 1 2 5 7 9\n",
    "```\n",
    "shows three stems (20s, 30s, 40s) with 4–5 values each. The data is fairly evenly spread across the three decades, with no strong clustering. Unlike a histogram, every original value is preserved exactly.\n",
    "\n",
    "**Key observations:**\n",
    "- **Range:** minimum = 23, maximum = 49\n",
    "- **Shape:** approximately uniform distribution across stems\n",
    "- **Data integrity:** all 14 observations are retained exactly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseball-histogram'></a>\n",
    "## 2. Discrete Histogram – Baseball Hits\n",
    "\n",
    "### Section 2.4 – Graphical Summaries\n",
    "\n",
    "A **histogram** divides data into bins and displays their frequencies. For grouped discrete data (like hit intervals in baseball), a bar chart serves as the discrete analogue.\n",
    "\n",
    "**Dataset:** Number of hits per team per nine-inning game (1989–1993), grouped into class intervals:\n",
    "\n",
    "| Hits | Frequency |\n",
    "|------|-----------|\n",
    "| 0–2  | 120       |\n",
    "| 3–4  | 380       |\n",
    "| 5–6  | 650       |\n",
    "| 7–8  | 720       |\n",
    "| 9–10 | 450       |\n",
    "| 11+  | 180       |\n",
    "\n",
    "Each bar's height is proportional to the count of games in that hit range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class intervals and frequencies\n",
    "hits_intervals = [\"0-2\", \"3-4\", \"5-6\", \"7-8\", \"9-10\", \"11+\"]\n",
    "frequencies = [120, 380, 650, 720, 450, 180]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(hits_intervals, frequencies, color='steelblue', edgecolor='black', alpha=0.85)\n",
    "plt.xlabel(\"Number of Hits per Game\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.title(\"Histogram of Baseball Hits per Team per Game (1989–1993)\", fontsize=13)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "The histogram shows a clear peak at **7–8 hits** per game, indicating this is the most typical offensive output. The distribution increases from left to the peak and then gradually declines, producing a slight **right skew** — unusually high-hit games occur less often but extend farther from the center.\n",
    "\n",
    "Approximately 64% of all games resulted in between 5 and 10 hits, reflecting a fairly concentrated central region.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='continuous-histogram'></a>\n",
    "## 3. Continuous Histogram – Bin Selection Rules\n",
    "\n",
    "### Section 2.4 – Graphical Summaries\n",
    "\n",
    "For continuous data, the number of bins affects how well a histogram reveals the underlying distribution shape. Two common rules are:\n",
    "\n",
    "- **Square-Root Rule:** $k = \\lceil \\sqrt{n} \\rceil$ — general-purpose, gives finer resolution\n",
    "- **Sturges' Rule:** $k = \\lceil \\log_2 n + 1 \\rceil$ — more conservative, suited for near-normal distributions\n",
    "\n",
    "The code below:\n",
    "1. Generates 100 synthetic response times from a log-normal distribution (positive-skewed, realistic)\n",
    "2. Computes the number of bins using each rule\n",
    "3. Plots side-by-side histograms for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# 1. Generate synthetic dataset (n=100)\n",
    "# Using a log-normal distribution to simulate realistic response times\n",
    "np.random.seed(42)\n",
    "data = np.random.lognormal(mean=2, sigma=0.5, size=100)\n",
    "n = len(data)\n",
    "x_min, x_max = data.min(), data.max()\n",
    "\n",
    "# 2. Calculate bins using the Square-Root Rule\n",
    "k_sqrt = math.ceil(math.sqrt(n))\n",
    "\n",
    "# 3. Calculate bins using Sturges' Rule\n",
    "k_sturges = math.ceil(math.log2(n) + 1)\n",
    "\n",
    "# Print calculations for verification\n",
    "print(f\"Dataset Size (n): {n}\")\n",
    "print(f\"Square-Root Rule: k = {k_sqrt}\")\n",
    "print(f\"Sturges' Rule:    k = {k_sturges}\")\n",
    "\n",
    "# 4. Plotting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# Plot Square-Root Rule Histogram\n",
    "axes[0].hist(data, bins=k_sqrt, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'Square-Root Rule (k={k_sqrt})', fontsize=13)\n",
    "axes[0].set_xlabel('Response Time (ms)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Sturges' Rule Histogram\n",
    "axes[1].hist(data, bins=k_sturges, color='salmon', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f\"Sturges' Rule (k={k_sturges})\", fontsize=13)\n",
    "axes[1].set_xlabel('Response Time (ms)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparison of Histogram Binning Strategies (n=100)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "| Feature | Square-Root Rule | Sturges' Rule |\n",
    "|---------|-----------------|---------------|\n",
    "| Bins (k) | 10 | 8 |\n",
    "| Resolution | Higher (finer) | Lower (smoother) |\n",
    "| Best Use | General exploration | Near-normal distributions |\n",
    "\n",
    "- The **Square-Root Rule** (k=10) produces a finer histogram with more detail, which can reveal subtle structure in the data.\n",
    "- **Sturges' Rule** (k=8) produces a smoother histogram that emphasizes the global shape.\n",
    "\n",
    "Both histograms reveal the same right-skewed, log-normal shape. The bin count is a trade-off between **bias** (too few bins oversmoothes) and **variance** (too many bins amplifies noise).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dotplot'></a>\n",
    "## 4. Dot Plot – Response Times\n",
    "\n",
    "### Section 2.4 – Graphical Summaries\n",
    "\n",
    "A **dot plot** represents each observation as a dot above its value on a horizontal scale. When multiple values are equal, dots are stacked vertically. This makes dot plots ideal for:\n",
    "- Small datasets (n < 100)\n",
    "- Preserving exact values (no binning)\n",
    "- Identifying clusters, gaps, and potential outliers\n",
    "\n",
    "**Dataset:** `[12, 12, 13, 15, 15, 15, 16, 18, 20, 20, 15, 12, 22]`\n",
    "\n",
    "The value 15 appears 4 times, so it produces the tallest stack — behaving as the mode in this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Sample Dataset\n",
    "data = [12, 12, 13, 15, 15, 15, 16, 18, 20, 20, 15, 12, 22]\n",
    "\n",
    "# 2. Function to calculate vertical stacks for dots\n",
    "def create_dotplot(data):\n",
    "    unique_values, counts = np.unique(data, return_counts=True)\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for val, count in zip(unique_values, counts):\n",
    "        for i in range(count):\n",
    "            x_coords.append(val)\n",
    "            y_coords.append(i + 1)  # Start stack at y=1\n",
    "    return x_coords, y_coords\n",
    "\n",
    "# Generate coordinates\n",
    "x, y = create_dotplot(data)\n",
    "\n",
    "# 3. Plotting\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x, y, s=100, color='royalblue', edgecolor='black')\n",
    "\n",
    "# Clean up the visual\n",
    "plt.yticks([])  # Hide Y-axis as height represents frequency\n",
    "plt.xlabel('Response Time (ms)', fontsize=12)\n",
    "plt.title('Dot Plot of Response Times', fontsize=13)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "plt.ylim(0, max(y) + 1)  # Give some space at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "from collections import Counter\n",
    "print(\"Value counts:\", dict(sorted(Counter(data).items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "The dot plot reveals:\n",
    "- **Mode = 15** — the tallest stack (4 dots), indicating the most frequent value\n",
    "- **12** appears 3 times (second most common)\n",
    "- **22** is an isolated dot to the right — a potential mild outlier\n",
    "- The distribution is roughly centered between 12 and 20 with a slight right tail\n",
    "\n",
    "Unlike a histogram, every individual data point is visible, making dot plots excellent for verifying data quality and spotting anomalies in small datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boxplot'></a>\n",
    "## 5. Box Plot – High-Variance Response Times\n",
    "\n",
    "### Section 2.4 – Graphical Summaries\n",
    "\n",
    "A **boxplot** (box-and-whisker plot) summarizes a distribution using the **five-number summary**:\n",
    "- Minimum (within fences)\n",
    "- Q1 (25th percentile)\n",
    "- Median (50th percentile)\n",
    "- Q3 (75th percentile)\n",
    "- Maximum (within fences)\n",
    "\n",
    "Points beyond $Q1 - 1.5 \\times \\text{IQR}$ or $Q3 + 1.5 \\times \\text{IQR}$ are plotted as individual outliers.\n",
    "\n",
    "**Dataset:** `[10, 12, 15, 25, 30, 35, 40, 45, 50, 60, 65, 70, 110, 120]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dataset with increased spread (larger IQR)\n",
    "data = [10, 12, 15, 25, 30, 35, 40, 45, 50, 60, 65, 70, 110, 120]\n",
    "\n",
    "# Compute five-number summary\n",
    "q1 = np.percentile(data, 25)\n",
    "median = np.median(data)\n",
    "q3 = np.percentile(data, 75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"  Minimum:               {min(data)}\")\n",
    "print(f\"  Q1 (25th percentile):  {q1}\")\n",
    "print(f\"  Median (50th):         {median}\")\n",
    "print(f\"  Q3 (75th percentile):  {q3}\")\n",
    "print(f\"  Maximum:               {max(data)}\")\n",
    "print(f\"  IQR:                   {iqr}\")\n",
    "print(f\"  Lower fence:           {q1 - 1.5 * iqr}\")\n",
    "print(f\"  Upper fence:           {q3 + 1.5 * iqr}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.boxplot(data, vert=False, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightgreen', color='darkgreen'),\n",
    "            medianprops=dict(color='red', linewidth=2),\n",
    "            flierprops=dict(marker='o', markerfacecolor='orange', markersize=8))\n",
    "plt.title('Box Plot of High-Variance Response Times', fontsize=13)\n",
    "plt.xlabel('Time (ms)', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Median = 42.5 ms** — the center of the dataset\n",
    "- **IQR = 35 ms** — the wide box reflects substantial spread in the middle 50% of observations\n",
    "- **No outliers** — the upper fence is 120.0, so all points (including 110 and 120) fall within the whiskers\n",
    "- The whiskers extend to 10 ms (minimum) and 120 ms (maximum)\n",
    "\n",
    "The wider the box, the more variable the middle portion of the data. Boxplots are especially useful for **comparing multiple groups** side by side.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mean-trimmed'></a>\n",
    "## 6. Sample Mean and Trimmed Mean\n",
    "\n",
    "### Section 2.5 – Numerical Summaries: Location\n",
    "\n",
    "The **sample mean** is computed as:\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$$\n",
    "\n",
    "It uses all data values but is sensitive to outliers.\n",
    "\n",
    "The **trimmed mean** removes a proportion $\\alpha$ of the smallest and largest observations before computing the average:\n",
    "$$\\bar{x}_{\\text{trim}} = \\frac{1}{n - 2k} \\sum_{i=k+1}^{n-k} x_{(i)}, \\quad k = \\lfloor \\alpha n \\rfloor$$\n",
    "\n",
    "A **20% trimmed mean** with n=7 removes 1 observation from each end (k = ⌊0.2 × 7⌋ = 1).\n",
    "\n",
    "**Dataset:** Test scores `[75, 82, 90, 68, 95, 88, 72]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data\n",
    "scores = np.array([75, 82, 90, 68, 95, 88, 72])\n",
    "\n",
    "# Sample mean\n",
    "sample_mean = np.mean(scores)\n",
    "\n",
    "# 20% trimmed mean\n",
    "trimmed_mean = stats.trim_mean(scores, 0.2)\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(f\"\\nSample mean:       {round(sample_mean, 2)}\")\n",
    "print(f\"20% trimmed mean:  {round(trimmed_mean, 2)}\")\n",
    "\n",
    "# Show which values are trimmed\n",
    "sorted_scores = np.sort(scores)\n",
    "k = int(0.2 * len(scores))\n",
    "trimmed_scores = sorted_scores[k:len(sorted_scores)-k]\n",
    "print(f\"\\nSorted scores:     {sorted_scores}\")\n",
    "print(f\"Removed (low):     {sorted_scores[:k]}\")\n",
    "print(f\"Removed (high):    {sorted_scores[len(sorted_scores)-k:]}\")\n",
    "print(f\"Remaining:         {trimmed_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Sample mean = 81.43** — computed using all 7 values including the extremes (68 and 95)\n",
    "- **20% trimmed mean = 81.4** — computed after removing 68 (lowest) and 95 (highest)\n",
    "\n",
    "In this balanced dataset the two measures are nearly identical. However, in skewed data or data with outliers, the trimmed mean will differ significantly from the sample mean, providing a more robust estimate of the center.\n",
    "\n",
    "The trimmed mean is a compromise between the sample mean (efficient but sensitive) and the median (robust but uses little data).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='percentiles'></a>\n",
    "## 7. Percentiles, Quartiles, and IQR\n",
    "\n",
    "### Section 2.5 – Numerical Summaries: Location\n",
    "\n",
    "**Quartiles** divide the ordered dataset into four equal parts:\n",
    "- **Q1** (25th percentile): 25% of data falls at or below this value\n",
    "- **Q2** (50th percentile, Median): 50% of data falls at or below\n",
    "- **Q3** (75th percentile): 75% of data falls at or below\n",
    "\n",
    "The **Interquartile Range (IQR)** measures the spread of the middle 50%:\n",
    "$$\\text{IQR} = Q3 - Q1$$\n",
    "\n",
    "**Dataset:** Exam scores `[62, 68, 71, 75, 78, 82, 86, 90]` (n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([62, 68, 71, 75, 78, 82, 86, 90])\n",
    "\n",
    "Q1 = np.percentile(data, 25, method=\"midpoint\")\n",
    "Q2 = np.percentile(data, 50, method=\"midpoint\")\n",
    "Q3 = np.percentile(data, 75, method=\"midpoint\")\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Dataset:\", data)\n",
    "print(f\"\\nQ1 (25th percentile): {Q1}\")\n",
    "print(f\"Q2 (Median):          {Q2}\")\n",
    "print(f\"Q3 (75th percentile): {Q3}\")\n",
    "print(f\"Interquartile Range:  {IQR}\")\n",
    "\n",
    "# Visual summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 3))\n",
    "bp = ax.boxplot(data, vert=False, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', color='navy'),\n",
    "                medianprops=dict(color='red', linewidth=2),\n",
    "                whiskerprops=dict(color='navy'),\n",
    "                capprops=dict(color='navy'))\n",
    "\n",
    "# Annotate\n",
    "for label, val, ypos in [(f'Q1={Q1}', Q1, 1.35), (f'Med={Q2}', Q2, 1.35),\n",
    "                          (f'Q3={Q3}', Q3, 1.35)]:\n",
    "    ax.annotate(label, xy=(val, 1), xytext=(val, ypos),\n",
    "                ha='center', fontsize=9,\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax.set_xlabel('Exam Score', fontsize=12)\n",
    "ax.set_title(f'Box Plot of Exam Scores — IQR = {IQR}', fontsize=13)\n",
    "ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Q1 = 69.5** — computed as the midpoint of the lower half (68 and 71)\n",
    "- **Q2 = 76.5** — the median, midpoint of 75 and 78\n",
    "- **Q3 = 84.0** — computed as the midpoint of the upper half (82 and 86)\n",
    "- **IQR = 14.5** — the middle 50% of exam scores spans 14.5 points\n",
    "\n",
    "The IQR is a robust measure of spread: it ignores the extreme minimum (62) and maximum (90), focusing on the central mass of the distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='variance-std'></a>\n",
    "## 8. Variance and Standard Deviation\n",
    "\n",
    "### Section 2.5 – Numerical Summaries: Variability\n",
    "\n",
    "The **sample variance** measures the average squared deviation from the mean:\n",
    "$$s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2$$\n",
    "\n",
    "The **sample standard deviation** is the square root of variance:\n",
    "$$s = \\sqrt{s^2}$$\n",
    "\n",
    "The divisor $n-1$ (Bessel's correction) makes $s^2$ an **unbiased estimator** of the population variance $\\sigma^2$.\n",
    "\n",
    "**Dataset:** `[4, 6, 8, 10, 12]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([4, 6, 8, 10, 12])\n",
    "\n",
    "mean = np.mean(data)\n",
    "variance = np.var(data, ddof=1)   # sample variance (ddof=1 applies Bessel's correction)\n",
    "std_dev  = np.std(data, ddof=1)   # sample standard deviation\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(f\"Mean:                     {mean}\")\n",
    "print(f\"Sample Variance (s²):     {variance}\")\n",
    "print(f\"Sample Std Deviation (s): {std_dev:.2f}\")\n",
    "\n",
    "# Show step-by-step deviations\n",
    "print(\"\\nStep-by-step deviations:\")\n",
    "print(f\"{'x_i':>5}  {'x_i - mean':>12}  {'(x_i - mean)^2':>16}\")\n",
    "for x in data:\n",
    "    dev = x - mean\n",
    "    sq  = dev ** 2\n",
    "    print(f\"{x:>5}  {dev:>12.1f}  {sq:>16.1f}\")\n",
    "print(f\"{'Sum':>5}  {'':>12}  {sum((x - mean)**2 for x in data):>16.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Mean = 8.0** — the balance point of the dataset\n",
    "- **Sample Variance = 10.0** — average squared deviation from the mean (in units²)\n",
    "- **Sample Std Dev = 3.16** — observations typically deviate about **3.16 units** from the mean\n",
    "\n",
    "The standard deviation is interpretable in the original units (unlike variance), making it the preferred spread measure. For approximately normal data, about 68% of observations fall within $\\bar{x} \\pm s$ (5 to 11 in this case).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='location-variability'></a>\n",
    "## 9. Location and Variability Comparison\n",
    "\n",
    "### Section 2.5 – Numerical Summaries\n",
    "\n",
    "Two datasets can share the **same mean and median** but differ dramatically in spread. This demonstrates why reporting both location **and** variability is essential.\n",
    "\n",
    "The **Coefficient of Variation (CV)** provides a scale-free measure of relative variability:\n",
    "$$\\text{CV} = \\frac{s}{\\bar{x}} \\times 100\\%$$\n",
    "\n",
    "**Datasets:**\n",
    "- `data_low_var  = [48, 49, 50, 51, 52]` — tightly clustered\n",
    "- `data_high_var = [30, 40, 50, 60, 70]` — widely spread\n",
    "\n",
    "Both have mean = 50, but their spreads differ by a factor of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Two datasets: same mean, different variability\n",
    "data_low_var  = np.array([48, 49, 50, 51, 52])\n",
    "data_high_var = np.array([30, 40, 50, 60, 70])\n",
    "\n",
    "def summarize(data, name):\n",
    "    std = np.std(data, ddof=1)\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean:    {np.mean(data):.2f}\")\n",
    "    print(f\"  Median:  {np.median(data):.2f}\")\n",
    "    print(f\"  Range:   {np.ptp(data):.2f}\")\n",
    "    print(f\"  IQR:     {q3 - q1:.2f}\")\n",
    "    print(f\"  Std Dev: {std:.2f}\")\n",
    "    print(f\"  CV:      {(std / np.mean(data)) * 100:.2f}%\")\n",
    "\n",
    "summarize(data_low_var,  \"Low Variability Dataset\")\n",
    "summarize(data_high_var, \"High Variability Dataset\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(data_low_var,  bins=5, alpha=0.6, label='Low Var',  edgecolor='black', color='steelblue')\n",
    "axes[0].hist(data_high_var, bins=5, alpha=0.6, label='High Var', edgecolor='black', color='salmon')\n",
    "axes[0].axvline(50, linestyle='--', linewidth=2, color='red', label='Mean = 50')\n",
    "axes[0].set_xlabel('Value', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Same Mean, Different Spread', fontsize=13)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].boxplot([data_low_var, data_high_var], labels=['Low Var', 'High Var'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightgray'))\n",
    "axes[1].set_ylabel('Value', fontsize=11)\n",
    "axes[1].set_title('Boxplot Comparison', fontsize=13)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "| Measure | Low Variability | High Variability |\n",
    "|---------|----------------|------------------|\n",
    "| Mean    | 50.00          | 50.00            |\n",
    "| Median  | 50.00          | 50.00            |\n",
    "| Range   | 4.00           | 40.00            |\n",
    "| IQR     | 2.50           | 25.00            |\n",
    "| Std Dev | 1.58           | 15.81            |\n",
    "| CV      | 3.16%          | 31.62%           |\n",
    "\n",
    "Despite identical means and medians, the high-variability dataset has a CV **10× larger**. In machine learning:\n",
    "- Features with high CV dominate distance calculations in algorithms like k-NN and k-means unless standardized\n",
    "- This motivates **feature scaling** (standardization) before training distance-based models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='skewness'></a>\n",
    "## 10. Skewness – Exam Scores\n",
    "\n",
    "### Section 2.6 – Shape, Skewness, and Outliers\n",
    "\n",
    "**Skewness** measures asymmetry of a distribution using the standardized third central moment:\n",
    "$$\\text{Skew} = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3$$\n",
    "\n",
    "- **Skew > 0:** right tail is longer (positive/right skew; mean > median)\n",
    "- **Skew < 0:** left tail is longer (negative/left skew; mean < median)\n",
    "- **Skew ≈ 0:** symmetric distribution\n",
    "\n",
    "**Dataset:** Exam scores `[40, 45, 50, 55, 60, 65, 90]` — the outlier 90 creates a long right tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([40, 45, 50, 55, 60, 65, 90])\n",
    "n = len(data)\n",
    "\n",
    "# Step 1: Mean\n",
    "mean = np.mean(data)\n",
    "\n",
    "# Step 2: Sample standard deviation\n",
    "std = np.std(data, ddof=1)\n",
    "\n",
    "# Step 3: Standardized values\n",
    "z = (data - mean) / std\n",
    "\n",
    "# Step 4: Cube standardized values\n",
    "z_cubed = z**3\n",
    "\n",
    "# Step 5: Sum of cubes\n",
    "sum_z_cubed = np.sum(z_cubed)\n",
    "\n",
    "# Step 6: Finite-sample corrected skewness\n",
    "skewness = (n / ((n - 1) * (n - 2))) * sum_z_cubed\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(f\"Mean:                       {round(mean, 4)}\")\n",
    "print(f\"Sample Standard Deviation:  {round(std, 4)}\")\n",
    "print(f\"Standardized values (z):    {np.round(z, 4)}\")\n",
    "print(f\"Cubed z values:             {np.round(z_cubed, 4)}\")\n",
    "print(f\"Sum of cubed z:             {round(sum_z_cubed, 4)}\")\n",
    "print(f\"Sample Skewness (manual):   {round(skewness, 4)}\")\n",
    "print(f\"Scipy skewness (bias=False):{round(skew(data, bias=False), 4)}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(len(data)), sorted(data), color='steelblue', edgecolor='black', alpha=0.8)\n",
    "axes[0].axhline(mean, color='red', linestyle='--', label=f'Mean={mean:.1f}')\n",
    "axes[0].axhline(np.median(data), color='green', linestyle='-.', label=f'Median={np.median(data)}')\n",
    "axes[0].set_title('Sorted Exam Scores', fontsize=12)\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(data, bins=7, color='salmon', edgecolor='black', alpha=0.8)\n",
    "axes[1].axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean={mean:.1f}')\n",
    "axes[1].axvline(np.median(data), color='green', linestyle='-.', linewidth=2, label=f'Median={np.median(data)}')\n",
    "axes[1].set_title(f'Histogram — Skewness = {round(skewness, 2)}', fontsize=12)\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Sample Skewness ≈ 1.30** — strongly positive (right-skewed)\n",
    "- **Mean (57.86) > Median (55)** — the outlier score of 90 pulls the mean rightward\n",
    "- The long right tail is caused by the single high score of 90\n",
    "\n",
    "**ML implications of skewed features:**\n",
    "- Heavily skewed features may benefit from **log transformation** (for positive skew)\n",
    "- Skewed prediction targets may require **asymmetric loss functions**\n",
    "- Skewed residuals suggest **model misspecification**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kurtosis'></a>\n",
    "## 11. Kurtosis – Transaction Amounts\n",
    "\n",
    "### Section 2.6 – Shape, Skewness, and Outliers\n",
    "\n",
    "**Kurtosis** measures tail heaviness — how prone a distribution is to producing extreme values (outliers).\n",
    "\n",
    "Most software (including scipy) reports **excess kurtosis** (Kurt − 3, where 3 is the Gaussian baseline):\n",
    "- **Excess Kurt ≈ 0:** mesokurtic (normal-like)\n",
    "- **Excess Kurt > 0:** leptokurtic (heavier tails, more outliers)\n",
    "- **Excess Kurt < 0:** platykurtic (lighter tails, fewer outliers)\n",
    "\n",
    "**Dataset:** Transaction amounts `[48, 49, 50, 50, 51, 52, 120]` — most values cluster tightly around 50, with one large outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [48, 49, 50, 50, 51, 52, 120]\n",
    "\n",
    "# Compute bias-corrected excess kurtosis (Fisher's definition, bias=False)\n",
    "kurt_value = kurtosis(data, bias=False)\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(f\"Mean:                {np.mean(data):.4f}\")\n",
    "print(f\"Std Dev:             {np.std(data, ddof=1):.4f}\")\n",
    "print(f\"Sample Kurtosis (G₂): {kurt_value:.4f}\")\n",
    "print()\n",
    "if kurt_value > 0:\n",
    "    print(\"→ LEPTOKURTIC: Heavy tails, more outlier-prone than normal distribution\")\n",
    "elif kurt_value < 0:\n",
    "    print(\"→ PLATYKURTIC: Light tails, fewer outliers than normal distribution\")\n",
    "else:\n",
    "    print(\"→ MESOKURTIC: Similar tail behavior to normal distribution\")\n",
    "\n",
    "# Visual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(len(data)), sorted(data), color='orchid', edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Sorted Transaction Amounts', fontsize=12)\n",
    "axes[0].set_ylabel('Amount')\n",
    "axes[0].set_xlabel('Rank')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(data, bins=8, color='orchid', edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title(f'Histogram — Kurtosis = {kurt_value:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel('Transaction Amount')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "- **Sample Kurtosis ≈ 6.95** — strongly leptokurtic (heavy tails)\n",
    "- Most transactions cluster tightly near 50, with the extreme value of 120 creating a very heavy right tail\n",
    "- The high kurtosis reflects a **high propensity for extreme values**\n",
    "\n",
    "**Real-world significance:**\n",
    "- Financial returns exhibit high kurtosis — rare but large price movements occur more often than Gaussian models predict\n",
    "- Sensor data with high kurtosis suggests occasional large errors requiring robust estimation\n",
    "- Models assuming normality (e.g., linear regression) may underestimate tail risk in high-kurtosis data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outlier'></a>\n",
    "## 12. Outlier Detection – IQR and Z-Score Methods\n",
    "\n",
    "### Section 2.6 – Shape, Skewness, and Outliers\n",
    "\n",
    "Two standard methods for flagging outliers:\n",
    "\n",
    "**1. Tukey's IQR Fences (robust):**\n",
    "$$\\text{Lower fence} = Q1 - 1.5 \\times \\text{IQR}, \\quad \\text{Upper fence} = Q3 + 1.5 \\times \\text{IQR}$$\n",
    "Values beyond the fences are flagged as outliers. This method is robust because it is based on quartiles, not the mean.\n",
    "\n",
    "**2. Z-Score Method (assumes near-normality):**\n",
    "$$z_i = \\frac{x_i - \\bar{x}}{\\sigma}$$\n",
    "Values with $|z_i| > 3$ (strict) or $|z_i| > 2$ (relaxed) are flagged.\n",
    "\n",
    "**Dataset:** `[48, 49, 50, 51, 52, 120]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.array([48, 49, 50, 51, 52, 120])\n",
    "\n",
    "# --- Tukey quartiles (manual split method) ---\n",
    "lower_half = np.array([48, 49, 50])\n",
    "upper_half = np.array([51, 52, 120])\n",
    "Q1 = np.median(lower_half)\n",
    "Q3 = np.median(upper_half)\n",
    "IQR = Q3 - Q1\n",
    "lower_fence = Q1 - 1.5 * IQR\n",
    "upper_fence = Q3 + 1.5 * IQR\n",
    "outliers_iqr = data[(data < lower_fence) | (data > upper_fence)]\n",
    "\n",
    "# --- Z-score method (population std) ---\n",
    "mean = np.mean(data)\n",
    "std = np.std(data, ddof=0)  # population standard deviation\n",
    "z_scores = (data - mean) / std\n",
    "outliers_z2 = data[np.abs(z_scores) > 2]\n",
    "outliers_z3 = data[np.abs(z_scores) > 3]\n",
    "\n",
    "print(\"Data:\", data)\n",
    "print(\"\\n--- IQR Method (Tukey) ---\")\n",
    "print(f\"Q1:           {Q1}\")\n",
    "print(f\"Q3:           {Q3}\")\n",
    "print(f\"IQR:          {IQR}\")\n",
    "print(f\"Lower fence:  {lower_fence}\")\n",
    "print(f\"Upper fence:  {upper_fence}\")\n",
    "print(f\"Outliers:     {outliers_iqr}\")\n",
    "\n",
    "print(\"\\n--- Z-Score Method (Population Std) ---\")\n",
    "print(f\"Mean:         {round(mean, 2)}\")\n",
    "print(f\"Std Dev:      {round(std, 2)}\")\n",
    "print(f\"Z-scores:     {np.round(z_scores, 2)}\")\n",
    "print(f\"Outliers (|z|>2): {outliers_z2}\")\n",
    "print(f\"Outliers (|z|>3): {outliers_z3}\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# IQR boxplot\n",
    "axes[0].boxplot(data, vert=False, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue'),\n",
    "                flierprops=dict(marker='D', markerfacecolor='red', markersize=10))\n",
    "axes[0].axvline(lower_fence, color='orange', linestyle='--', label=f'Lower fence={lower_fence}')\n",
    "axes[0].axvline(upper_fence, color='red',    linestyle='--', label=f'Upper fence={upper_fence}')\n",
    "axes[0].set_title('IQR Outlier Detection', fontsize=12)\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# Z-score bar chart\n",
    "colors = ['red' if abs(z) > 2 else 'steelblue' for z in z_scores]\n",
    "axes[1].bar(data, np.abs(z_scores), color=colors, edgecolor='black', alpha=0.85, width=3)\n",
    "axes[1].axhline(2, color='orange', linestyle='--', label='|z|=2 threshold')\n",
    "axes[1].axhline(3, color='red',    linestyle='--', label='|z|=3 threshold')\n",
    "axes[1].set_title('Z-Score Magnitudes', fontsize=12)\n",
    "axes[1].set_xlabel('Data Value')\n",
    "axes[1].set_ylabel('|Z-Score|')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "| Method | Outliers Detected |\n",
    "|--------|------------------|\n",
    "| IQR (Tukey, upper fence=56.5) | **120** |\n",
    "| Z-score (\\|z\\|>2) | **120** |\n",
    "| Z-score (\\|z\\|>3) | None |\n",
    "\n",
    "- The **IQR method** is more sensitive for small samples with extreme values — it correctly flags 120\n",
    "- Under the strict **z>3 rule**, 120 is not flagged because the extreme value itself inflates the mean and std\n",
    "- **IQR-based methods are preferred** for small samples and skewed distributions\n",
    "\n",
    "**Handling outliers:**\n",
    "1. **Investigate** — is it a data entry error or a genuine observation?\n",
    "2. **Transform** — log transform can reduce outlier impact\n",
    "3. **Robust methods** — use median/IQR instead of mean/std\n",
    "4. **Winsorization** — cap extreme values at fence boundaries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sampling-dist'></a>\n",
    "## 13. Sampling Distributions Simulation\n",
    "\n",
    "### Section 2.7 – From Samples to Populations\n",
    "\n",
    "A **sampling distribution** is the probability distribution of a statistic (e.g., sample mean $\\bar{X}$) computed over all possible samples of size $n$ from the same population.\n",
    "\n",
    "Key properties of the sampling distribution of $\\bar{X}$:\n",
    "1. **Mean:** $E[\\bar{X}] = \\mu$ — sample means are centered at the true population mean\n",
    "2. **Standard Error:** $\\text{SE}(\\bar{X}) = \\sigma / \\sqrt{n}$ — precision increases with sample size\n",
    "3. **Shape:** Approximately normal for large $n$ (Central Limit Theorem)\n",
    "\n",
    "This simulation:\n",
    "- Creates a population of 100,000 values (normal, $\\mu=50$, $\\sigma=10$)\n",
    "- For each of three sample sizes (n=5, 25, 100), draws 1,000 samples and records the sample mean\n",
    "- Plots the resulting sampling distributions to show how precision increases with n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Population parameters\n",
    "population_mean = 50\n",
    "population_std  = 10\n",
    "population_size = 100000\n",
    "\n",
    "# Create population\n",
    "population = np.random.normal(population_mean, population_std, population_size)\n",
    "\n",
    "# Function to simulate sampling distribution\n",
    "def simulate_sampling_distribution(population, sample_size, num_samples=1000):\n",
    "    sample_means = []\n",
    "    for _ in range(num_samples):\n",
    "        sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "        sample_means.append(np.mean(sample))\n",
    "    return np.array(sample_means)\n",
    "\n",
    "# Simulate for different sample sizes\n",
    "sample_sizes = [5, 25, 100]\n",
    "colors = ['cornflowerblue', 'tomato', 'mediumseagreen']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    sample_means = simulate_sampling_distribution(population, n)\n",
    "    theoretical_se = population_std / np.sqrt(n)\n",
    "\n",
    "    plt.hist(sample_means, bins=50, alpha=0.55, color=colors[i], density=True,\n",
    "             label=f'n={n}  |  Theoretical SE={theoretical_se:.2f}  |  Empirical SD={np.std(sample_means):.2f}')\n",
    "\n",
    "    print(f\"Sample size n={n}:\")\n",
    "    print(f\"  Theoretical SE:             {theoretical_se:.3f}\")\n",
    "    print(f\"  Empirical SD of sample means: {np.std(sample_means):.3f}\")\n",
    "    print(f\"  Mean of sample means:         {np.mean(sample_means):.3f}\")\n",
    "    print()\n",
    "\n",
    "plt.axvline(x=population_mean, color='black', linestyle='--', linewidth=2,\n",
    "            label=f'Population mean μ = {population_mean}')\n",
    "plt.xlabel('Sample Mean $\\\\bar{x}$', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Sampling Distributions of the Sample Mean for Different Sample Sizes', fontsize=13)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Interpretation\n",
    "\n",
    "| Sample Size | Theoretical SE | Empirical SD | Mean of Sample Means |\n",
    "|-------------|---------------|--------------|---------------------|\n",
    "| n=5         | 4.472         | ≈ 4.44       | ≈ 50.0              |\n",
    "| n=25        | 2.000         | ≈ 1.98       | ≈ 50.0              |\n",
    "| n=100       | 1.000         | ≈ 0.99       | ≈ 50.0              |\n",
    "\n",
    "**Key conclusions:**\n",
    "\n",
    "1. **All sampling distributions are centered at μ=50** regardless of sample size — confirming $\\bar{X}$ is an unbiased estimator of $\\mu$\n",
    "2. **Larger n → smaller SE** — the distribution narrows as $\\text{SE} = \\sigma/\\sqrt{n}$\n",
    "3. **Empirical SDs match theoretical SEs** — the simulation validates the mathematical formula\n",
    "4. **To halve the SE, you must quadruple n** (e.g., n=25→n=100 cuts SE from 2.0 to 1.0)\n",
    "\n",
    "**ML connection:** Training data is a sample from the data-generating distribution. Sampling variability explains why model accuracy estimates vary across different train-test splits. Larger test sets yield more reliable estimates of true model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "This notebook covered the core statistical foundations needed for machine learning:\n",
    "\n",
    "| Topic | Key Concepts |\n",
    "|-------|--------------|\n",
    "| Graphical Tools | Stem-and-leaf, histograms (bin rules), dot plots, box plots |\n",
    "| Location Measures | Mean, trimmed mean, median, mode, percentiles, quartiles |\n",
    "| Variability Measures | Range, variance, std dev, IQR, coefficient of variation |\n",
    "| Shape Measures | Skewness (asymmetry), kurtosis (tail heaviness) |\n",
    "| Outlier Detection | Tukey's IQR fences, Z-score thresholds |\n",
    "| Sampling | Sampling variability, standard error, sampling distributions |\n",
    "\n",
    "> **Remember:** Always visualize data before computing numerical summaries. Graphics reveal patterns, outliers, and distributional shape that numbers alone obscure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
