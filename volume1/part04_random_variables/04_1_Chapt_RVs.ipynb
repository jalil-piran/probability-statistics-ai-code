{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2715044a",
   "metadata": {},
   "source": [
    "# Probability & Statistics for AI — Computational Examples\n",
    "\n",
    "This notebook reproduces and explains all four Python examples from the chapter on\n",
    "**distributions, transformations, and Monte Carlo methods**.\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 1 | Empirical Distributions, KDE, and Q-Q Plots |\n",
    "| 2 | Transformations and Change of Variables |\n",
    "| 3 | Functions of Random Variables |\n",
    "| 4 | Monte Carlo Sampling Methods |\n",
    "\n",
    "**Dependencies:** `numpy`, `matplotlib`, `scipy`, `seaborn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import quad\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"All libraries loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f402ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Empirical Distributions, KDE, and Q-Q Plots\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "When the true data-generating distribution is unknown, nonparametric tools let us\n",
    "characterise the data without committing to a parametric family.\n",
    "\n",
    "**Empirical CDF (ECDF)**  \n",
    "Given $n$ i.i.d. observations $X_1,\\ldots,X_n$, the empirical CDF is\n",
    "$$\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}(X_i \\le x).$$\n",
    "The Glivenko–Cantelli theorem guarantees uniform convergence:\n",
    "$\\sup_x |\\hat{F}_n(x) - F(x)| \\xrightarrow{a.s.} 0$.\n",
    "The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality gives a finite-sample\n",
    "confidence band: with probability $\\ge 1-\\delta$,\n",
    "$$\\sup_x |\\hat{F}_n(x) - F(x)| \\le \\sqrt{\\frac{\\log(2/\\delta)}{2n}}.$$\n",
    "\n",
    "**Kernel Density Estimation (KDE)**  \n",
    "The KDE replaces each observation with a smooth kernel, producing a density estimate\n",
    "$$\\hat{f}_h(x) = \\frac{1}{nh}\\sum_{i=1}^n K\\!\\left(\\frac{x - X_i}{h}\\right).$$\n",
    "Bandwidth $h$ controls the bias–variance trade-off: small $h$ gives high variance\n",
    "(spiky), large $h$ gives high bias (oversmoothed).  Scott's and Silverman's rules\n",
    "provide data-driven defaults for Gaussian data.\n",
    "\n",
    "**Q-Q Plots**  \n",
    "A quantile-quantile (Q-Q) plot graphs sample quantiles against theoretical quantiles\n",
    "of a reference distribution.  If the data follow that distribution the points fall on\n",
    "the diagonal $y = x$.  Systematic curvature signals heavy tails, skewness, or a\n",
    "distributional mismatch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9c430",
   "metadata": {},
   "source": [
    "### 1.1  Data Generation: Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59314538",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# 60% from N(2, 0.8²), 40% from N(7, 1.2²)\n",
    "component = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])\n",
    "data = np.where(\n",
    "    component == 0,\n",
    "    np.random.normal(2, 0.8, n_samples),\n",
    "    np.random.normal(7, 1.2, n_samples)\n",
    ")\n",
    "\n",
    "print(f\"Generated {n_samples} samples from a two-component Gaussian mixture.\")\n",
    "print(f\"  Component weights : 60% N(2, 0.8²)  |  40% N(7, 1.2²)\")\n",
    "print(f\"  Sample mean       : {data.mean():.3f}\")\n",
    "print(f\"  Sample std        : {data.std():.3f}\")\n",
    "\n",
    "# True density on a fine grid (used for comparison in every plot)\n",
    "x_grid   = np.linspace(-2, 11, 500)\n",
    "true_pdf = (0.6 * stats.norm.pdf(x_grid, 2, 0.8) +\n",
    "            0.4 * stats.norm.pdf(x_grid, 7, 1.2))\n",
    "true_cdf = (0.6 * stats.norm.cdf(x_grid, 2, 0.8) +\n",
    "            0.4 * stats.norm.cdf(x_grid, 7, 1.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8ecda",
   "metadata": {},
   "source": [
    "A **Gaussian mixture** is used throughout this section because it is bimodal —\n",
    "any method that assumes unimodality will visibly fail, making the comparison\n",
    "between parametric and nonparametric approaches instructive.\n",
    "The true density is stored on a grid for overlay comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12617781",
   "metadata": {},
   "source": [
    "### 1.2  Empirical CDF and DKW Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_cdf(x_eval, samples):\n",
    "    \"\"\"Evaluate the empirical CDF at each point in x_eval.\"\"\"\n",
    "    n = len(samples)\n",
    "    return np.array([np.sum(samples <= x) / n for x in x_eval])\n",
    "\n",
    "ecdf_vals     = empirical_cdf(x_grid, data)\n",
    "max_deviation = np.max(np.abs(ecdf_vals - true_cdf))\n",
    "dkw_bound_95  = np.sqrt(np.log(2 / 0.05) / (2 * n_samples))\n",
    "\n",
    "print(\"Empirical CDF — Glivenko–Cantelli check\")\n",
    "print(f\"  Max deviation |F̂ₙ - F|    = {max_deviation:.4f}\")\n",
    "print(f\"  DKW 95% confidence bound   = {dkw_bound_95:.4f}\")\n",
    "print(f\"  Deviation within bound?    : {max_deviation < dkw_bound_95}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5aafcd",
   "metadata": {},
   "source": [
    "**Interpreting the output:**  \n",
    "The maximum absolute deviation between the ECDF and the true CDF is well below the\n",
    "DKW bound, confirming that $n=200$ is sufficient for the ECDF to be a reliable\n",
    "proxy for the true CDF.  As $n$ grows the bound shrinks as $1/\\sqrt{n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d776a68",
   "metadata": {},
   "source": [
    "### 1.3  Kernel Density Estimation — Bandwidth Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9504f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidths = {\n",
    "    'Small  h=0.15': 0.15,\n",
    "    \"Scott's rule\" : 'scott',\n",
    "    \"Silverman's\"  : 'silverman',\n",
    "    'Large  h=0.80': 0.80,\n",
    "}\n",
    "\n",
    "kde_estimates = {}\n",
    "print(\"KDE bandwidths:\")\n",
    "for name, bw in bandwidths.items():\n",
    "    kde = gaussian_kde(data, bw_method=bw)\n",
    "    kde_estimates[name] = kde(x_grid)\n",
    "    actual_bw = kde.factor * data.std() if isinstance(bw, str) else bw\n",
    "    print(f\"  {name:<20s}: h = {actual_bw:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf63d6",
   "metadata": {},
   "source": [
    "**Interpreting the output:**  \n",
    "Scott's rule ($h \\propto n^{-1/5}$) and Silverman's rule ($h \\propto n^{-1/5}$, slightly\n",
    "larger) are automatic bandwidth selectors calibrated for roughly Gaussian data.\n",
    "For this bimodal sample they perform reasonably well, sitting between the\n",
    "undersmoothed spiky estimate ($h=0.15$) and the oversmoothed flat estimate ($h=0.80$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402efeac",
   "metadata": {},
   "source": [
    "### 1.4  Q-Q Plot: Assessing Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b024c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = np.sort(data)\n",
    "n = len(sorted_data)\n",
    "\n",
    "# Plotting positions: p_i = i/(n+1)  (Blom's approximation avoids p=0 or 1)\n",
    "plot_positions = np.arange(1, n + 1) / (n + 1)\n",
    "\n",
    "# Theoretical quantiles of a fitted Gaussian\n",
    "mean_est = data.mean()\n",
    "std_est  = data.std()\n",
    "gaussian_q = stats.norm.ppf(plot_positions, mean_est, std_est)\n",
    "\n",
    "corr = np.corrcoef(sorted_data, gaussian_q)[0, 1]\n",
    "print(f\"Q-Q correlation (data vs fitted Gaussian): {corr:.4f}\")\n",
    "print(f\"A value well below 1 signals non-Gaussianity (expected for a bimodal mixture).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879eba2a",
   "metadata": {},
   "source": [
    "### 1.5  Combined Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f09044",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ── Panel 1: ECDF vs true CDF ────────────────────────────────────────────\n",
    "axes[0, 0].plot(x_grid, true_cdf, 'r-',  lw=2, label='True CDF')\n",
    "axes[0, 0].plot(x_grid, ecdf_vals,'b-',  lw=2, alpha=0.8, label='Empirical CDF')\n",
    "axes[0, 0].fill_between(x_grid,\n",
    "                         true_cdf - dkw_bound_95, true_cdf + dkw_bound_95,\n",
    "                         alpha=0.15, color='red', label='DKW 95% band')\n",
    "axes[0, 0].set_title('ECDF vs True CDF with DKW Band', fontsize=11)\n",
    "axes[0, 0].set_xlabel('x'); axes[0, 0].set_ylabel('CDF')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 2: KDE bandwidth comparison ────────────────────────────────────\n",
    "colors = ['steelblue','tomato','mediumseagreen','darkorange']\n",
    "for (name, kde_est), col in zip(kde_estimates.items(), colors):\n",
    "    axes[0, 1].plot(x_grid, kde_est, color=col, lw=1.8, label=name)\n",
    "axes[0, 1].plot(x_grid, true_pdf, 'k--', lw=2, label='True PDF')\n",
    "axes[0, 1].set_title('KDE: Bandwidth Effect', fontsize=11)\n",
    "axes[0, 1].set_xlabel('x'); axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "\n",
    "# ── Panel 3: Q-Q plot ─────────────────────────────────────────────────────\n",
    "axes[1, 0].scatter(gaussian_q, sorted_data, alpha=0.6, s=20, color='steelblue')\n",
    "ref = np.linspace(data.min(), data.max(), 200)\n",
    "axes[1, 0].plot(ref, ref, 'r--', lw=1.5, label='y = x (perfect fit)')\n",
    "axes[1, 0].set_title(f'Q-Q Plot: Data vs Gaussian (r = {corr:.3f})', fontsize=11)\n",
    "axes[1, 0].set_xlabel('Theoretical quantiles'); axes[1, 0].set_ylabel('Sample quantiles')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 4: Histogram + best KDE overlay ────────────────────────────────\n",
    "best_kde = kde_estimates[\"Scott's rule\"]\n",
    "axes[1, 1].hist(data, bins=25, density=True, alpha=0.45,\n",
    "                color='steelblue', label='Histogram')\n",
    "axes[1, 1].plot(x_grid, best_kde,  'b-', lw=2, label=\"KDE (Scott)\")\n",
    "axes[1, 1].plot(x_grid, true_pdf,  'r--', lw=2, label='True PDF')\n",
    "axes[1, 1].set_title(\"Histogram vs KDE vs True PDF\", fontsize=11)\n",
    "axes[1, 1].set_xlabel('x'); axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Empirical Distributions, KDE, and Q-Q Plots', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('empirical_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved → empirical_distributions.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728cdc1",
   "metadata": {},
   "source": [
    "**Reading the figure:**\n",
    "\n",
    "- *Top-left*: The ECDF closely tracks the true CDF and stays inside the DKW band,\n",
    "  confirming the Glivenko–Cantelli guarantee in practice.\n",
    "- *Top-right*: The under-smoothed KDE ($h=0.15$) produces spurious spikes; the\n",
    "  over-smoothed estimate ($h=0.80$) washes out the bimodal structure.  Scott's and\n",
    "  Silverman's rules preserve both modes reasonably well.\n",
    "- *Bottom-left*: The Q-Q points deviate from the diagonal near the centre, where the\n",
    "  gap between the two modes creates an S-shaped pattern.  This is the characteristic\n",
    "  Q-Q signature of a bimodal (or heavy-tailed) distribution being compared to a\n",
    "  Gaussian reference.\n",
    "- *Bottom-right*: The KDE using Scott's rule closely matches the true bimodal PDF,\n",
    "  outperforming the histogram which is sensitive to bin width and placement.\n",
    "\n",
    "**Key insights**  \n",
    "1. The ECDF converges uniformly to the true CDF (Glivenko–Cantelli); the DKW bound quantifies the finite-sample error.  \n",
    "2. KDE bandwidth controls the bias–variance trade-off; automatic rules work well for roughly unimodal data.  \n",
    "3. Q-Q plots expose non-Gaussianity visually — any systematic departure from the diagonal is diagnostically informative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50f244",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Transformations and Change of Variables\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "**Linear transformations**  \n",
    "If $Y = aX + b$ then $E[Y] = aE[X] + b$ and $\\text{Var}(Y) = a^2\\,\\text{Var}(X)$.\n",
    "Crucially, a linear transformation of a Gaussian random variable remains Gaussian.\n",
    "\n",
    "**Standardisation (Z-score)**  \n",
    "Setting $Z = (X - \\mu)/\\sigma$ produces $E[Z] = 0$ and $\\text{Var}(Z) = 1$.\n",
    "This is the special case $a = 1/\\sigma$, $b = -\\mu/\\sigma$.\n",
    "\n",
    "**Change of variables (Jacobian)**  \n",
    "For a strictly monotone function $g$, the PDF transforms as\n",
    "$$f_Y(y) = f_X\\!\\left(g^{-1}(y)\\right)\\cdot\\left|\\frac{d}{dy}g^{-1}(y)\\right|.$$\n",
    "The term $|d g^{-1}/dy|$ is the **Jacobian** — it corrects for the stretching or\n",
    "compression of probability mass.\n",
    "\n",
    "**Log-normal distribution**  \n",
    "If $X \\sim N(0,1)$ then $Y = e^X$ is log-normal: $Y \\sim \\text{LogNormal}(0,1)$.\n",
    "The Jacobian at $y$ is $|dx/dy| = 1/y$, giving\n",
    "$f_Y(y) = f_X(\\ln y)/y$.\n",
    "\n",
    "**Probability Integral Transform (PIT)**  \n",
    "For any continuous random variable $X$ with CDF $F$, the random variable $U = F(X)$\n",
    "follows a $\\text{Uniform}(0,1)$ distribution.  The inverse transform $X = F^{-1}(U)$\n",
    "converts uniform samples into samples from $F$ — the foundation of inverse transform\n",
    "sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a49b27",
   "metadata": {},
   "source": [
    "### 2.1  Linear Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "mu, sigma = 2, 1.5\n",
    "n_samples = 5_000\n",
    "\n",
    "X = np.random.normal(mu, sigma, n_samples)\n",
    "a, b = 3, 5\n",
    "Y = a * X + b\n",
    "\n",
    "print(f\"Original   X ~ N({mu}, {sigma}²)\")\n",
    "print(f\"  E[X]   = {X.mean():.4f}    (theory: {mu})\")\n",
    "print(f\"  Var(X) = {X.var():.4f}   (theory: {sigma**2:.4f})\")\n",
    "\n",
    "print(f\"\\nTransformed  Y = {a}X + {b}  (linear)\")\n",
    "print(f\"  E[Y]   = {Y.mean():.4f}    (theory: {a*mu + b})\")\n",
    "print(f\"  Var(Y) = {Y.var():.4f}   (theory: {a**2 * sigma**2:.4f})\")\n",
    "print(f\"  SD(Y)  = {Y.std():.4f}    (theory: {abs(a)*sigma:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acaff9",
   "metadata": {},
   "source": [
    "### 2.2  Standardisation (Z-score) and Batch Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae72337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Z-score standardisation ───────────────────────────────────────────────\n",
    "Z = (X - X.mean()) / X.std()\n",
    "print(f\"Standardised Z = (X - x̄) / s\")\n",
    "print(f\"  E[Z]   = {Z.mean():.6f}  (should be ~0)\")\n",
    "print(f\"  Var(Z) = {Z.var():.6f}  (should be ~1)\")\n",
    "\n",
    "# ── Batch normalisation (neural-network context) ──────────────────────────\n",
    "batch_size, n_features = 32, 10\n",
    "activations = np.random.randn(batch_size, n_features) * 10 + 5\n",
    "\n",
    "epsilon    = 1e-5\n",
    "batch_mean = activations.mean(axis=0, keepdims=True)\n",
    "batch_var  = activations.var(axis=0,  keepdims=True)\n",
    "normalised = (activations - batch_mean) / np.sqrt(batch_var + epsilon)\n",
    "\n",
    "# Learned affine re-scaling\n",
    "gamma = np.ones(n_features)   # scale  (learned; set to 1 here)\n",
    "beta_param  = np.zeros(n_features)  # shift (learned; set to 0 here)\n",
    "bn_output   = gamma * normalised + beta_param\n",
    "\n",
    "print(f\"\\nBatch normalisation (first 3 features):\")\n",
    "print(f\"  Mean  after BN: {normalised.mean(axis=0)[:3].round(4)}\")\n",
    "print(f\"  Std   after BN: {normalised.std(axis=0)[:3].round(4)}\")\n",
    "print(\"  => Each feature is centred and scaled within each mini-batch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b977231",
   "metadata": {},
   "source": [
    "**Batch normalisation** applies exactly the same Gaussian standardisation to each\n",
    "feature column of a mini-batch.  This reduces *internal covariate shift* — the\n",
    "change in the distribution of layer inputs as network weights update — which allows\n",
    "higher learning rates and more stable training.  The learnable parameters $\\gamma$\n",
    "(scale) and $\\beta$ (shift) restore the network's expressive capacity after\n",
    "normalisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d1c28b",
   "metadata": {},
   "source": [
    "### 2.3  Nonlinear Transformation: Exponential → Log-Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = np.random.normal(0, 1, 5_000)\n",
    "Y_lognorm = np.exp(X_std)\n",
    "\n",
    "# Jacobian-based PDF derivation at y = 2\n",
    "y_test  = 2.0\n",
    "x_test  = np.log(y_test)                          # g^{-1}(y)\n",
    "pdf_x   = stats.norm.pdf(x_test, 0, 1)\n",
    "jacobian = 1.0 / y_test                           # |dx/dy| = 1/y\n",
    "pdf_y_manual = pdf_x * jacobian\n",
    "pdf_y_scipy  = stats.lognorm.pdf(y_test, s=1, scale=1)\n",
    "\n",
    "print(\"Exponential transformation  Y = exp(X),  X ~ N(0,1)\")\n",
    "print(f\"  f_Y(2) via Jacobian : {pdf_y_manual:.6f}\")\n",
    "print(f\"  f_Y(2) via SciPy    : {pdf_y_scipy:.6f}\")\n",
    "print(f\"  Difference          : {abs(pdf_y_manual - pdf_y_scipy):.2e}\")\n",
    "print(\"\\nSample moments of Y:\")\n",
    "print(f\"  E[Y]   ≈ {Y_lognorm.mean():.4f}  (theory: e^0.5 = {np.exp(0.5):.4f})\")\n",
    "print(f\"  Var(Y) ≈ {Y_lognorm.var():.4f}  (theory: (e-1)e = {(np.e-1)*np.e:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361538cf",
   "metadata": {},
   "source": [
    "### 2.4  Probability Integral Transform (PIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8115393",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pit, sigma_pit = 3, 2\n",
    "X_gauss = np.random.normal(mu_pit, sigma_pit, 5_000)\n",
    "\n",
    "# Forward PIT: U = F(X) should be Uniform(0,1)\n",
    "U_forward = stats.norm.cdf(X_gauss, mu_pit, sigma_pit)\n",
    "print(\"Probability Integral Transform:\")\n",
    "print(f\"  U = F(X)  →  mean = {U_forward.mean():.4f}  (expected 0.5)\")\n",
    "print(f\"              std  = {U_forward.std():.4f}  (expected {1/np.sqrt(12):.4f})\")\n",
    "\n",
    "# Inverse PIT: X = F^{-1}(U) recovers the original distribution\n",
    "U_uniform   = np.random.uniform(0, 1, 5_000)\n",
    "X_recovered = stats.norm.ppf(U_uniform, mu_pit, sigma_pit)\n",
    "print(f\"\\nInverse PIT (sampling):  X = F⁻¹(U)\")\n",
    "print(f\"  Recovered mean = {X_recovered.mean():.4f}  (expected {mu_pit})\")\n",
    "print(f\"  Recovered std  = {X_recovered.std():.4f}  (expected {sigma_pit})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b66a",
   "metadata": {},
   "source": [
    "### 2.5  Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "# ── Panel 1: Linear transformation ───────────────────────────────────────\n",
    "bins = 60\n",
    "axes[0, 0].hist(X, bins=bins, density=True, alpha=0.5, color='steelblue', label=f'X ~ N({mu},{sigma}²)')\n",
    "axes[0, 0].hist(Y, bins=bins, density=True, alpha=0.5, color='tomato',    label=f'Y={a}X+{b}')\n",
    "# Overlay true densities\n",
    "x_rng = np.linspace(X.min()-1, Y.max()+1, 400)\n",
    "axes[0, 0].plot(x_rng, stats.norm.pdf(x_rng, mu, sigma), 'b-', lw=2)\n",
    "axes[0, 0].plot(x_rng, stats.norm.pdf(x_rng, a*mu+b, abs(a)*sigma), 'r-', lw=2)\n",
    "axes[0, 0].set_title('Linear Transformation  Y = 3X + 5', fontsize=11)\n",
    "axes[0, 0].set_xlabel('Value'); axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 2: Log-normal ───────────────────────────────────────────────────\n",
    "y_grid = np.linspace(0.01, 10, 500)\n",
    "axes[0, 1].hist(Y_lognorm, bins=60, density=True, alpha=0.5, color='mediumseagreen')\n",
    "axes[0, 1].plot(y_grid, stats.lognorm.pdf(y_grid, s=1, scale=1), 'r-', lw=2, label='LogNormal(0,1)')\n",
    "axes[0, 1].set_title('Y = exp(X),  X ~ N(0,1)  →  Log-Normal', fontsize=11)\n",
    "axes[0, 1].set_xlabel('y'); axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 3: PIT uniformity check ────────────────────────────────────────\n",
    "axes[1, 0].hist(U_forward, bins=40, density=True, alpha=0.6, color='darkorchid')\n",
    "axes[1, 0].axhline(1.0, color='red', lw=2, ls='--', label='Uniform(0,1) density = 1')\n",
    "axes[1, 0].set_title('PIT: U = F(X) ~ Uniform(0,1)', fontsize=11)\n",
    "axes[1, 0].set_xlabel('u'); axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 4: Z-score before / after ──────────────────────────────────────\n",
    "axes[1, 1].hist(X, bins=50, density=True, alpha=0.5, color='steelblue', label='Original X')\n",
    "axes[1, 1].hist(Z, bins=50, density=True, alpha=0.5, color='tomato',    label='Standardised Z')\n",
    "z_range = np.linspace(-4, 4, 300)\n",
    "axes[1, 1].plot(z_range, stats.norm.pdf(z_range), 'k-', lw=2, label='N(0,1)')\n",
    "axes[1, 1].set_title('Standardisation: Z = (X − μ) / σ', fontsize=11)\n",
    "axes[1, 1].set_xlabel('Value'); axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Transformations and Change of Variables', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('transformations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved → transformations.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b83f6f",
   "metadata": {},
   "source": [
    "**Reading the figure:**\n",
    "\n",
    "- *Top-left*: The linear transformation shifts and scales the Gaussian; both histograms\n",
    "  sit under their respective theoretical densities, confirming $E[Y]=11$ and $\\text{SD}(Y)=4.5$.\n",
    "- *Top-right*: The log-normal is right-skewed with a mode well below the mean —\n",
    "  the consequence of compressing the left tail of the Gaussian through $e^x$.\n",
    "  The Jacobian-derived density (red) matches the histogram perfectly.\n",
    "- *Bottom-left*: After applying the PIT, the histogram of $U = F(X)$ is flat\n",
    "  across $[0,1]$, confirming the transform produces a uniform distribution regardless\n",
    "  of the original distribution $F$.\n",
    "- *Bottom-right*: Standardisation centres and scales $X$; the resulting $Z$\n",
    "  histogram aligns with the standard normal density.\n",
    "\n",
    "**Key insights**  \n",
    "1. Linear: $E[Y] = aE[X]+b$, $\\text{Var}(Y) = a^2\\text{Var}(X)$.  \n",
    "2. Standardisation scales to mean 0 and variance 1 — the basis of batch normalisation in deep learning.  \n",
    "3. Nonlinear transformations require the Jacobian determinant to convert PDFs correctly.  \n",
    "4. The PIT converts any continuous CDF to Uniform$(0,1)$; the inverse converts uniforms to any target distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd1ee80",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Functions of Random Variables\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "**Discrete case**  \n",
    "For $Y = g(X)$ with $X$ discrete, the PMF of $Y$ accumulates probability mass from\n",
    "all inputs that map to the same output:\n",
    "$$P(Y = y) = \\sum_{x:\\,g(x)=y} P(X = x).$$\n",
    "\n",
    "**Continuous case — change of variables**  \n",
    "For a strictly monotone differentiable $g$:\n",
    "$$f_Y(y) = f_X\\!\\left(g^{-1}(y)\\right)\\cdot\\left|\\frac{d}{dy}g^{-1}(y)\\right|.$$\n",
    "For non-monotone $g$ (e.g. squaring), sum over all pre-images.\n",
    "\n",
    "**Chi-squared distribution**  \n",
    "If $Z \\sim N(0,1)$ then $Y = Z^2 \\sim \\chi^2(1)$.  The PDF is derived by\n",
    "splitting the pre-image at $\\pm\\sqrt{y}$:\n",
    "$$f_Y(y) = \\frac{1}{\\sqrt{2\\pi y}}\\,e^{-y/2}, \\quad y > 0.$$\n",
    "\n",
    "**Reproductive properties**  \n",
    "Many families are closed under addition of independent members:\n",
    "- Gaussians: $X_1 + X_2 \\sim N(\\mu_1+\\mu_2,\\,\\sigma_1^2+\\sigma_2^2)$.\n",
    "- Poisson: sum of independent Poisson variables is Poisson.\n",
    "- Chi-squared: sum of $k$ independent $\\chi^2(1)$ variables is $\\chi^2(k)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4871cd",
   "metadata": {},
   "source": [
    "### 3.1  Discrete: Absolute Value Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba426979",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# X is uniform on {-2, -1, 0, 1, 2}\n",
    "X_support = np.array([-2, -1, 0, 1, 2])\n",
    "X_pmf     = np.ones(5) / 5\n",
    "\n",
    "# Y = |X|: two values of X map to each positive y\n",
    "Y_values  = np.abs(X_support)\n",
    "Y_support = np.unique(Y_values)\n",
    "Y_pmf     = np.array([X_pmf[Y_values == y].sum() for y in Y_support])\n",
    "\n",
    "print(\"X ~ Uniform{-2, -1, 0, 1, 2}\")\n",
    "for x, p in zip(X_support, X_pmf):\n",
    "    print(f\"  P(X = {x:+d}) = {p:.3f}\")\n",
    "\n",
    "print(\"\\nY = |X|  (collapsing ±k onto k)\")\n",
    "for y, p in zip(Y_support, Y_pmf):\n",
    "    print(f\"  P(Y = {y}) = {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76369a8d",
   "metadata": {},
   "source": [
    "The absolute value collapses each symmetric pair $\\{-k, +k\\}$ onto $k$,\n",
    "doubling its probability.  The origin $X=0$ has no symmetric partner, so its\n",
    "probability is unchanged.  This is the simplest illustration of the many-to-one\n",
    "accumulation rule for discrete transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451e007",
   "metadata": {},
   "source": [
    "### 3.2  Discrete: Sum of Two Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb490fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 36 equally likely outcomes for a pair of fair dice\n",
    "d1, d2 = np.meshgrid(range(1, 7), range(1, 7))\n",
    "sums    = (d1 + d2).flatten()\n",
    "\n",
    "sum_support = np.arange(2, 13)\n",
    "sum_pmf     = np.array([(sums == s).sum() / 36 for s in sum_support])\n",
    "\n",
    "print(\"S = D₁ + D₂  (sum of two fair dice)\")\n",
    "for s, p in zip(sum_support, sum_pmf):\n",
    "    bar = \"█\" * int(p * 200)\n",
    "    print(f\"  P(S={s:2d}) = {p:.4f}  {bar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b354f",
   "metadata": {},
   "source": [
    "### 3.3  Continuous: Squaring a Standard Normal → Chi-Squared(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0523e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10_000\n",
    "Z         = np.random.standard_normal(n_samples)\n",
    "Y_sq      = Z ** 2\n",
    "\n",
    "print(f\"Z ~ N(0,1),  Y = Z²\")\n",
    "print(f\"  E[Y]   empirical = {Y_sq.mean():.4f}   (theory: 1)\")\n",
    "print(f\"  Var(Y) empirical = {Y_sq.var():.4f}   (theory: 2)\")\n",
    "\n",
    "# Jacobian derivation at y = 1\n",
    "y_test     = 1.0\n",
    "sqrt_y     = np.sqrt(y_test)\n",
    "pdf_manual = (stats.norm.pdf(sqrt_y) + stats.norm.pdf(-sqrt_y)) / (2 * sqrt_y)\n",
    "pdf_scipy  = stats.chi2.pdf(y_test, df=1)\n",
    "\n",
    "print(f\"\\nPDF at y = 1:\")\n",
    "print(f\"  Jacobian method : {pdf_manual:.6f}\")\n",
    "print(f\"  SciPy χ²(1)    : {pdf_scipy:.6f}\")\n",
    "print(f\"  Agreement       : {abs(pdf_manual - pdf_scipy) < 1e-8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf28f91",
   "metadata": {},
   "source": [
    "### 3.4  Reproductive Property: Sum of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, sigma1 = 2, 1.5\n",
    "mu2, sigma2 = 3, 2.0\n",
    "\n",
    "X1 = np.random.normal(mu1, sigma1, 5_000)\n",
    "X2 = np.random.normal(mu2, sigma2, 5_000)\n",
    "S  = X1 + X2\n",
    "\n",
    "print(f\"X₁ ~ N({mu1}, {sigma1}²),  X₂ ~ N({mu2}, {sigma2}²)\")\n",
    "print(f\"S = X₁ + X₂:\")\n",
    "print(f\"  E[S]   = {S.mean():.4f}   (theory: {mu1+mu2})\")\n",
    "print(f\"  Var(S) = {S.var():.4f}   (theory: {sigma1**2 + sigma2**2:.4f})\")\n",
    "print(f\"  SD(S)  = {S.std():.4f}   (theory: {np.sqrt(sigma1**2+sigma2**2):.4f})\")\n",
    "print(\"  => Sum of independent Gaussians is Gaussian (reproductive property).\")\n",
    "\n",
    "# Chi-squared reproductive: sum of independent χ²(1) is χ²(k)\n",
    "k = 5\n",
    "Z_arr   = np.random.standard_normal((n_samples, k))\n",
    "chi2_k  = (Z_arr ** 2).sum(axis=1)\n",
    "print(f\"\\nSum of {k} independent χ²(1):  empirical mean = {chi2_k.mean():.4f}  (theory: {k})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a098c0",
   "metadata": {},
   "source": [
    "### 3.5  Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13500bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "# ── Panel 1: |X| transformation ──────────────────────────────────────────\n",
    "width = 0.35\n",
    "axes[0, 0].bar(X_support - width/2, X_pmf, width=width,\n",
    "               alpha=0.7, color='steelblue', label='X ~ Uniform{-2..2}')\n",
    "axes[0, 0].bar(Y_support + width/2, Y_pmf, width=width,\n",
    "               alpha=0.7, color='tomato',    label='Y = |X|')\n",
    "axes[0, 0].set_xticks([-2, -1, 0, 1, 2])\n",
    "axes[0, 0].set_title('Discrete: Y = |X|', fontsize=11)\n",
    "axes[0, 0].set_xlabel('Value'); axes[0, 0].set_ylabel('Probability')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 2: Dice sum ─────────────────────────────────────────────────────\n",
    "axes[0, 1].bar(sum_support, sum_pmf, color='mediumseagreen', alpha=0.8, edgecolor='white')\n",
    "axes[0, 1].set_xticks(sum_support)\n",
    "axes[0, 1].set_title('Sum of Two Fair Dice', fontsize=11)\n",
    "axes[0, 1].set_xlabel('Sum'); axes[0, 1].set_ylabel('Probability')\n",
    "\n",
    "# ── Panel 3: Chi-squared(1) ───────────────────────────────────────────────\n",
    "y_grid = np.linspace(0.02, 8, 500)\n",
    "axes[1, 0].hist(Y_sq, bins=80, density=True, alpha=0.5, color='steelblue', label='Z² samples')\n",
    "axes[1, 0].plot(y_grid, stats.chi2.pdf(y_grid, df=1), 'r-', lw=2.5, label='χ²(1) PDF')\n",
    "axes[1, 0].set_xlim(0, 8); axes[1, 0].set_ylim(0, 2)\n",
    "axes[1, 0].set_title('Z² ~ Chi-Squared(1)', fontsize=11)\n",
    "axes[1, 0].set_xlabel('y'); axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 4: Gaussian reproductive property ───────────────────────────────\n",
    "x_rng = np.linspace(-4, 12, 400)\n",
    "axes[1, 1].hist(X1, bins=50, density=True, alpha=0.4, color='steelblue', label=f'X₁~N({mu1},{sigma1}²)')\n",
    "axes[1, 1].hist(X2, bins=50, density=True, alpha=0.4, color='tomato',    label=f'X₂~N({mu2},{sigma2}²)')\n",
    "axes[1, 1].hist(S,  bins=50, density=True, alpha=0.4, color='mediumseagreen', label='S = X₁+X₂')\n",
    "mu_s   = mu1 + mu2\n",
    "sig_s  = np.sqrt(sigma1**2 + sigma2**2)\n",
    "axes[1, 1].plot(x_rng, stats.norm.pdf(x_rng, mu_s, sig_s), 'k-', lw=2,\n",
    "                label=f'Theory N({mu_s},{sig_s:.2f}²)')\n",
    "axes[1, 1].set_title('Reproductive Property: Gaussian Sum', fontsize=11)\n",
    "axes[1, 1].set_xlabel('Value'); axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Functions of Random Variables', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('functions_rv.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved → functions_rv.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf27d5c",
   "metadata": {},
   "source": [
    "**Reading the figure:**\n",
    "\n",
    "- *Top-left*: The PMF of $|X|$ reassigns the probability of $\\pm k$ to $k$,\n",
    "  doubling the mass at values 1 and 2 relative to the original uniform.\n",
    "- *Top-right*: The triangular shape of the dice-sum distribution is a consequence\n",
    "  of convolution: more ways to achieve middle values (e.g. 36 ways for sum = 7\n",
    "  is the mode) than extremes (one way each for 2 and 12).\n",
    "- *Bottom-left*: The $\\chi^2(1)$ density is strongly right-skewed with a pole near\n",
    "  zero; the sample histogram matches the theoretical curve closely.\n",
    "- *Bottom-right*: The sum of two independent Gaussians is again Gaussian.\n",
    "  The theoretical density $N(5, 6.25)$ (black line) sits perfectly over the\n",
    "  histogram of $S$.\n",
    "\n",
    "**Key insights**  \n",
    "1. Discrete: $p_Y(y) = \\sum_{x:\\,g(x)=y} p_X(x)$ — accumulate mass from all pre-images.  \n",
    "2. Continuous: PDF transforms via the Jacobian; non-monotone functions require summing over all pre-images.  \n",
    "3. Reproductive properties — closure under summation — are exploited throughout statistics (ANOVA, regression, hypothesis testing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e349df",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Monte Carlo Sampling Methods\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "When direct simulation from a target distribution $p(x)$ is difficult, several\n",
    "algorithmic strategies provide exact or approximate samples.\n",
    "\n",
    "**Inverse Transform Sampling**  \n",
    "If $U \\sim \\text{Uniform}(0,1)$ and $F$ is the CDF of the target, then $X = F^{-1}(U)$\n",
    "has distribution $F$.  This requires a closed-form quantile function — available for\n",
    "the exponential, but not for the Gaussian in closed form.\n",
    "\n",
    "**Box-Muller Transform**  \n",
    "Two independent $U_1, U_2 \\sim U(0,1)$ generate two independent $N(0,1)$ samples:\n",
    "$$Z_1 = \\sqrt{-2\\ln U_1}\\cos(2\\pi U_2), \\quad Z_2 = \\sqrt{-2\\ln U_1}\\sin(2\\pi U_2).$$\n",
    "\n",
    "**Rejection Sampling**  \n",
    "Envelope distribution $Mq(x) \\ge p(x)$ everywhere.  Propose $X \\sim q$; accept with\n",
    "probability $p(X)/(M q(X))$.  Acceptance rate is $1/M$.  Exact but inefficient\n",
    "when $M$ is large (high-dimensional problems).\n",
    "\n",
    "**Importance Sampling**  \n",
    "Estimate $E_p[h(X)]$ by sampling from a proposal $q$ and reweighting:\n",
    "$$E_p[h(X)] \\approx \\frac{1}{n}\\sum_{i=1}^n h(x_i)\\,\\frac{p(x_i)}{q(x_i)}, \\quad x_i \\sim q.$$\n",
    "Critical for rare-event estimation where naive MC yields zero samples.\n",
    "\n",
    "**Metropolis–Hastings (MH) MCMC**  \n",
    "Construct a Markov chain with stationary distribution $p$.  At each step propose\n",
    "$x' \\sim q(\\cdot \\mid x_t)$; accept with probability $\\min(1, p(x')q(x_t\\mid x')/[p(x_t)q(x'\\mid x_t)])$.\n",
    "Requires only the unnormalised density.  Produces correlated samples; the\n",
    "**effective sample size (ESS)** $\\approx n/\\tau$ where $\\tau$ is the integrated\n",
    "autocorrelation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35f07b",
   "metadata": {},
   "source": [
    "### 4.1  Inverse Transform Sampling: Exponential Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 10_000\n",
    "\n",
    "def inverse_transform_exponential(n, lam=1.0):\n",
    "    \"\"\"\n",
    "    Generate Exponential(λ) samples using the inverse CDF:\n",
    "      F(x) = 1 - exp(-λx)  =>  F⁻¹(u) = -ln(1-u)/λ ≡ -ln(u)/λ\n",
    "    (using 1-U ~ U since U ~ Uniform avoids catastrophic cancellation near 1)\n",
    "    \"\"\"\n",
    "    U = np.random.uniform(0, 1, n)\n",
    "    return -np.log(U) / lam\n",
    "\n",
    "lam = 2.0\n",
    "exp_samples = inverse_transform_exponential(n_samples, lam)\n",
    "\n",
    "print(f\"Exponential(λ={lam}) via inverse transform  (n={n_samples})\")\n",
    "print(f\"  Sample mean : {exp_samples.mean():.4f}   (theory: 1/λ = {1/lam:.4f})\")\n",
    "print(f\"  Sample var  : {exp_samples.var():.4f}   (theory: 1/λ² = {1/lam**2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2d427",
   "metadata": {},
   "source": [
    "### 4.2  Box-Muller Transform: Gaussian Samples from Uniforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_muller(n):\n",
    "    \"\"\"\n",
    "    Generate two independent N(0,1) variates from two U(0,1) inputs.\n",
    "    Uses the polar form of the Box-Muller transform.\n",
    "    \"\"\"\n",
    "    U1 = np.random.uniform(0, 1, n)\n",
    "    U2 = np.random.uniform(0, 1, n)\n",
    "    R  = np.sqrt(-2 * np.log(U1))          # Rayleigh radius\n",
    "    Z1 = R * np.cos(2 * np.pi * U2)\n",
    "    Z2 = R * np.sin(2 * np.pi * U2)\n",
    "    return Z1, Z2\n",
    "\n",
    "Z1, Z2 = box_muller(n_samples)\n",
    "print(\"Standard Normal via Box-Muller\")\n",
    "print(f\"  Z1  mean={Z1.mean():.4f},  std={Z1.std():.4f}\")\n",
    "print(f\"  Z2  mean={Z2.mean():.4f},  std={Z2.std():.4f}\")\n",
    "print(f\"  Correlation(Z1, Z2) = {np.corrcoef(Z1, Z2)[0,1]:.4f}  (should be ~0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf0328",
   "metadata": {},
   "source": [
    "### 4.3  Rejection Sampling: Beta(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc70b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def rejection_sampling_beta(n, alpha=2, beta_p=5):\n",
    "    \"\"\"\n",
    "    Sample from Beta(alpha, beta_p) using Uniform(0,1) as proposal.\n",
    "    Envelope: M = max of Beta PDF over [0,1].\n",
    "    \"\"\"\n",
    "    beta_dist = stats.beta(alpha, beta_p)\n",
    "    result    = minimize_scalar(lambda x: -beta_dist.pdf(x),\n",
    "                                bounds=(0, 1), method='bounded')\n",
    "    M = beta_dist.pdf(result.x)          # envelope constant\n",
    "\n",
    "    samples, n_proposed = [], 0\n",
    "    while len(samples) < n:\n",
    "        X_prop = np.random.uniform(0, 1)\n",
    "        U      = np.random.uniform(0, 1)\n",
    "        n_proposed += 1\n",
    "        if U <= beta_dist.pdf(X_prop) / M:\n",
    "            samples.append(X_prop)\n",
    "\n",
    "    return np.array(samples), n / n_proposed, M\n",
    "\n",
    "beta_samples, acc_rate, M = rejection_sampling_beta(n_samples)\n",
    "print(f\"Beta(2, 5) via rejection sampling  (n={n_samples})\")\n",
    "print(f\"  Envelope constant M : {M:.4f}\")\n",
    "print(f\"  Acceptance rate     : {acc_rate:.4f}   (theory: 1/M = {1/M:.4f})\")\n",
    "print(f\"  Sample mean         : {beta_samples.mean():.4f}   (theory: α/(α+β) = {2/7:.4f})\")\n",
    "print(f\"  Sample var          : {beta_samples.var():.4f}   \"\n",
    "      f\"(theory: {2*5/(7**2 * 8):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f6501c",
   "metadata": {},
   "source": [
    "### 4.4  Importance Sampling: Rare Event Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3db72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_prob = 1 - stats.norm.cdf(4)\n",
    "print(f\"Target: P(Z > 4),  Z ~ N(0,1)\")\n",
    "print(f\"  True probability : {true_prob:.4e}\")\n",
    "\n",
    "# ── Naive Monte Carlo ─────────────────────────────────────────────────────\n",
    "naive_est = np.mean(np.random.standard_normal(n_samples) > 4)\n",
    "print(f\"  Naive MC (n={n_samples}) : {naive_est:.4e}  \"\n",
    "      f\"({'no hits' if naive_est == 0 else f'{int(naive_est*n_samples)} hits'})\")\n",
    "\n",
    "# ── Importance Sampling with proposal N(4, 1) ─────────────────────────────\n",
    "# Weight w(x) = p(x)/q(x) = φ(x) / φ_{N(4,1)}(x)\n",
    "prop_samples = np.random.normal(4, 1, n_samples)\n",
    "weights      = (stats.norm.pdf(prop_samples) /           # target density\n",
    "                stats.norm.pdf(prop_samples, 4, 1))      # proposal density\n",
    "is_est  = np.mean((prop_samples > 4) * weights)\n",
    "ess_is  = np.sum(weights)**2 / np.sum(weights**2)\n",
    "\n",
    "print(f\"  Importance sampling  : {is_est:.4e}\")\n",
    "print(f\"  Relative error       : {abs(is_est - true_prob)/true_prob * 100:.2f}%\")\n",
    "print(f\"  Effective sample size: {ess_is:.1f} / {n_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b953b",
   "metadata": {},
   "source": [
    "**Why importance sampling works here:**  \n",
    "Naive MC with $n=10{,}000$ rarely sees a value exceeding 4 (expected ~0.3 hits),\n",
    "so the estimate is almost always exactly 0.  By sampling from $N(4,1)$, which is\n",
    "centred at the threshold, essentially every proposal sample falls in the region of\n",
    "interest.  The likelihood ratio weight $w(x) = \\phi(x)/\\phi_{N(4,1)}(x)$ corrects\n",
    "for sampling from the wrong distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471d892",
   "metadata": {},
   "source": [
    "### 4.5  Metropolis–Hastings MCMC: Bimodal Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimodal_log_density(x):\n",
    "    \"\"\"log p(x) for  p(x) = 0.3 N(x;-2,1) + 0.7 N(x;2,1).\"\"\"    return np.log(0.3 * stats.norm.pdf(x, -2, 1) +\n",
    "                  0.7 * stats.norm.pdf(x,  2, 1))\n",
    "\n",
    "def metropolis_hastings(log_target, n, x0=0.0, proposal_std=1.0):\n",
    "    \"\"\"\n",
    "    Gaussian random-walk Metropolis-Hastings.\n",
    "    Returns chain and acceptance rate.\n",
    "    \"\"\"\n",
    "    chain       = np.zeros(n)\n",
    "    chain[0]    = x0\n",
    "    n_accepted  = 0\n",
    "\n",
    "    for t in range(1, n):\n",
    "        x_prop   = chain[t-1] + np.random.normal(0, proposal_std)\n",
    "        log_a    = log_target(x_prop) - log_target(chain[t-1])\n",
    "        if np.log(np.random.uniform()) < log_a:\n",
    "            chain[t] = x_prop\n",
    "            n_accepted += 1\n",
    "        else:\n",
    "            chain[t] = chain[t-1]\n",
    "\n",
    "    return chain, n_accepted / (n - 1)\n",
    "\n",
    "n_mcmc, burn_in = 20_000, 2_000\n",
    "chain, mh_acc_rate = metropolis_hastings(\n",
    "    bimodal_log_density, n_mcmc, x0=0.0, proposal_std=2.0)\n",
    "post_burnin = chain[burn_in:]\n",
    "\n",
    "# Integrated autocorrelation time → ESS\n",
    "def acf(x, max_lag=100):\n",
    "    xc   = x - x.mean()\n",
    "    full = np.correlate(xc, xc, mode='full')[len(x)-1:]\n",
    "    return full[:max_lag+1] / full[0]\n",
    "\n",
    "acf_vals = acf(post_burnin)\n",
    "tau      = 1 + 2 * np.sum(acf_vals[1:50])      # truncated sum\n",
    "ess_mcmc = len(post_burnin) / tau\n",
    "\n",
    "print(f\"Metropolis-Hastings — bimodal target: 0.3·N(-2,1) + 0.7·N(2,1)\")\n",
    "print(f\"  Acceptance rate          : {mh_acc_rate:.3f}\")\n",
    "print(f\"  Posterior mean (theory)  : {0.3*(-2)+0.7*2:.4f}\")\n",
    "print(f\"  Posterior mean (MCMC)    : {post_burnin.mean():.4f}\")\n",
    "print(f\"  Autocorrelation time τ   : {tau:.1f}\")\n",
    "print(f\"  Effective sample size    : {ess_mcmc:.1f}  / {len(post_burnin)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e3b80",
   "metadata": {},
   "source": [
    "### 4.6  Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 9))\n",
    "\n",
    "# ── Panel 1: Inverse transform — Exponential ──────────────────────────────\n",
    "x_eg = np.linspace(0, 4, 300)\n",
    "axes[0, 0].hist(exp_samples, bins=60, density=True, alpha=0.6, color='steelblue')\n",
    "axes[0, 0].plot(x_eg, stats.expon.pdf(x_eg, scale=1/lam), 'r-', lw=2, label='True PDF')\n",
    "axes[0, 0].set_title('Inverse Transform: Exponential(2)', fontsize=10)\n",
    "axes[0, 0].set_xlabel('x'); axes[0, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 2: Box-Muller — Gaussian ───────────────────────────────────────\n",
    "z_eg = np.linspace(-4, 4, 300)\n",
    "axes[0, 1].hist(Z1, bins=60, density=True, alpha=0.6, color='mediumseagreen')\n",
    "axes[0, 1].plot(z_eg, stats.norm.pdf(z_eg), 'r-', lw=2, label='N(0,1) PDF')\n",
    "axes[0, 1].set_title('Box-Muller: Standard Normal', fontsize=10)\n",
    "axes[0, 1].set_xlabel('z'); axes[0, 1].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 3: Rejection sampling — Beta ───────────────────────────────────\n",
    "b_eg = np.linspace(0, 1, 300)\n",
    "axes[0, 2].hist(beta_samples, bins=50, density=True, alpha=0.6, color='darkorange')\n",
    "axes[0, 2].plot(b_eg, stats.beta.pdf(b_eg, 2, 5), 'r-', lw=2, label='Beta(2,5) PDF')\n",
    "axes[0, 2].set_title(f'Rejection Sampling: Beta(2,5)\n",
    "acc. rate = {acc_rate:.3f}', fontsize=10)\n",
    "axes[0, 2].set_xlabel('x'); axes[0, 2].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 4: MCMC trace ───────────────────────────────────────────────────\n",
    "axes[1, 0].plot(chain[:3000], lw=0.5, alpha=0.8, color='steelblue')\n",
    "axes[1, 0].axvline(burn_in, color='red', ls='--', lw=1.5, label=f'Burn-in = {burn_in}')\n",
    "axes[1, 0].set_title('MCMC Trace (first 3000 steps)', fontsize=10)\n",
    "axes[1, 0].set_xlabel('Iteration'); axes[1, 0].set_ylabel('x')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 5: MCMC histogram vs target ────────────────────────────────────\n",
    "bg = np.linspace(-6, 6, 400)\n",
    "true_bimodal = 0.3 * stats.norm.pdf(bg, -2, 1) + 0.7 * stats.norm.pdf(bg, 2, 1)\n",
    "axes[1, 1].hist(post_burnin, bins=60, density=True, alpha=0.6, color='steelblue',\n",
    "                label='MCMC samples')\n",
    "axes[1, 1].plot(bg, true_bimodal, 'r-', lw=2, label='True PDF')\n",
    "axes[1, 1].set_title('MCMC: Bimodal Target (post burn-in)', fontsize=10)\n",
    "axes[1, 1].set_xlabel('x'); axes[1, 1].legend(fontsize=9)\n",
    "\n",
    "# ── Panel 6: Autocorrelation function ────────────────────────────────────\n",
    "lags = 50\n",
    "axes[1, 2].bar(range(lags), acf_vals[:lags], color='darkorchid', alpha=0.7)\n",
    "axes[1, 2].axhline(0,    color='k',   lw=0.8)\n",
    "axes[1, 2].axhline( 0.05, color='r', ls='--', lw=1, label='±5% threshold')\n",
    "axes[1, 2].axhline(-0.05, color='r', ls='--', lw=1)\n",
    "axes[1, 2].set_title(f'MCMC ACF  (ESS ≈ {ess_mcmc:.0f} / {len(post_burnin)})', fontsize=10)\n",
    "axes[1, 2].set_xlabel('Lag'); axes[1, 2].set_ylabel('ACF')\n",
    "axes[1, 2].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Monte Carlo Sampling Methods', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('monte_carlo_methods.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure saved → monte_carlo_methods.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb3a00",
   "metadata": {},
   "source": [
    "**Reading the figure:**\n",
    "\n",
    "- *Top-left*: Inverse transform generates exact exponential samples; the histogram\n",
    "  matches the theoretical density $f(x)=\\lambda e^{-\\lambda x}$ precisely.\n",
    "- *Top-middle*: Box-Muller samples align with the standard normal density — a\n",
    "  trigonometric construction that turns two uniform inputs into two independent\n",
    "  Gaussians.\n",
    "- *Top-right*: Rejection sampling correctly shapes the skewed Beta(2,5) density.\n",
    "  The acceptance rate (~0.45) equals $1/M$ as theory predicts.\n",
    "- *Bottom-left*: The trace plot shows rapid mixing after the burn-in period.\n",
    "  The chain oscillates between the two modes, indicating the proposal standard\n",
    "  deviation (2.0) is well-tuned for this target.\n",
    "- *Bottom-middle*: The post-burn-in histogram closely matches the bimodal target,\n",
    "  with each mode capturing the correct proportion of probability mass (30% and 70%).\n",
    "- *Bottom-right*: The ACF decays to the noise threshold within ~20 lags, so the\n",
    "  effective sample size is a substantial fraction of the total chain length.\n",
    "\n",
    "**Key insights**  \n",
    "1. Inverse transform: exact and efficient when the quantile function is available in closed form.  \n",
    "2. Box-Muller: turns pairs of uniforms into pairs of independent Gaussians via a geometric construction.  \n",
    "3. Rejection sampling: exact but the acceptance rate $1/M$ degrades exponentially in dimension.  \n",
    "4. Importance sampling: reweighting by the likelihood ratio concentrates effort on the rare-event region and eliminates zero-estimate failures.  \n",
    "5. Metropolis-Hastings: scales to high dimensions and requires only the unnormalised density; correlated samples reduce the effective sample size, quantified by the integrated autocorrelation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8039b",
   "metadata": {},
   "source": [
    "---\n",
    "## Overall Summary\n",
    "\n",
    "| Section | Core Concept | Key Formula |\n",
    "|---------|-------------|-------------|\n",
    "| Empirical distributions | ECDF convergence | $\\sup_x |\\hat{F}_n - F| \\le \\sqrt{\\log(2/\\delta)/(2n)}$ |\n",
    "| KDE | Bandwidth trade-off | $\\hat{f}_h(x) = n^{-1}h^{-1}\\sum_i K((x-X_i)/h)$ |\n",
    "| Q-Q plots | Distributional assessment | Points on diagonal ↔ correct family |\n",
    "| Linear transform | Mean/variance rules | $E[aX+b]=aE[X]+b$, $\\text{Var}(aX+b)=a^2\\text{Var}(X)$ |\n",
    "| Change of variables | Jacobian | $f_Y(y) = f_X(g^{-1}(y))\\cdot|dg^{-1}/dy|$ |\n",
    "| Discrete functions | Mass accumulation | $p_Y(y) = \\sum_{x:g(x)=y} p_X(x)$ |\n",
    "| Reproductive property | Closure under addition | Gaussians, Poisson, $\\chi^2$ |\n",
    "| Inverse transform | Exact sampling | $X = F^{-1}(U)$, $U\\sim U(0,1)$ |\n",
    "| Box-Muller | Gaussian sampling | $Z = \\sqrt{-2\\ln U_1}\\cos(2\\pi U_2)$ |\n",
    "| Rejection sampling | Exact but costly | Accept rate $= 1/M$ |\n",
    "| Importance sampling | Rare events | $E_p[h] \\approx n^{-1}\\sum h(x_i)\\,p(x_i)/q(x_i)$ |\n",
    "| Metropolis-Hastings | MCMC | Detailed balance; ESS $= n/\\tau$ |\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
